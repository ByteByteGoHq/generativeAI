# Reference Materials for Generative AI System Design Interview
## Chapter 1: Introduction and Overview
[1] Machine Learning System Design Interview. https://www.aliaminian.com/books.  
[2] Support Vector Machines. https://scikit-learn.org/stable/modules/svm.html.  
[3] Bayes’ theorem. https://en.wikipedia.org/wiki/Bayes%27_theorem.  
[4] Gaussian mixture models. https://scikit-learn.org/1.5/modules/mixture.html.  
[5] Hidden Markov model. https://en.wikipedia.org/wiki/Hidden_Markov_model.  
[6] Boltzmann machine. https://en.wikipedia.org/wiki/Boltzmann_machine.  
[7] OpenAI’s ChatGPT. https://openai.com/index/chatgpt/.  
[8] Economic Potential of Generative AI. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier.  
[9] The Llama 3 Herd of Models. https://arxiv.org/abs/2407.21783.  
[10] Flamingo: a Visual Language Model for Few-Shot Learning. https://arxiv.org/abs/2204.14198.  
[11] PaLM: Scaling Language Modeling with Pathways. https://arxiv.org/abs/2204.02311.  
[12] Language Models are Few-Shot Learners. https://arxiv.org/abs/2005.14165.  
[13] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. https://arxiv.org/abs/2205.11487.  
[14] PaLM2 Technical Report. https://arxiv.org/abs/2305.10403.  
[15] H100 Tensor Core GPU. https://www.nvidia.com/en-us/data-center/h100/.  
[16] GPT-4 training cost. www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/.  
[17] Scaling Laws for Neural Language Models. https://arxiv.org/abs/2001.08361.  
[18] Training Compute-Optimal Large Language Models. https://arxiv.org/abs/2203.15556.  
[19] Introducing OpenAI o1. https://openai.com/index/introducing-openai-o1-preview/.  
[20] Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. https://arxiv.org/abs/2407.21787.  
[21] ETL. https://aws.amazon.com/what-is/etl/.  
[22] Tecton. https://www.tecton.ai/feature-store/.  
[23] Amazon SageMaker. https://aws.amazon.com/sagemaker/.  
[24] ML System Design Interview. https://www.amazon.com/gp/product/1736049127/.  
[25] Comprehensive Exploration of Synthetic Data Generation: A Survey. https://arxiv.org/abs/2401.02524.  
[26] HDFS Architecture Guide. https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html.  
[27] Amazon S3. https://aws.amazon.com/s3/.  
[28] Apache Parquet. https://parquet.apache.org/.  
[29] Apache ORC. https://orc.apache.org/docs/.  
[30] Apache Lucene. https://lucene.apache.org/.  
[31] Elasticsearch. https://www.elastic.co/elasticsearch.  
[32] Attention Is All You Need. https://arxiv.org/abs/1706.03762.  
[33] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://arxiv.org/abs/1810.04805.  
[34] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. https://arxiv.org/abs/2010.11929.  
[35] Learning Transferable Visual Models From Natural Language Supervision. https://arxiv.org/abs/2103.00020.  
[36] Zero-Shot Text-to-Image Generation. https://arxiv.org/abs/2102.12092.  
[37] Neural Machine Translation by Jointly Learning to Align and Translate. https://arxiv.org/abs/1409.0473.  
[38] Common Crawl. https://commoncrawl.org/.  
[39] Data Parallelism. https://en.wikipedia.org/wiki/Data_parallelism.  
[40] Model Parallelism. https://huggingface.co/docs/transformers/v4.15.0/en/parallelism.  
[41] Pipeline Parallelism. https://pytorch.org/docs/stable/distributed.pipelining.html.  
[42] Mixed Precision Training. https://arxiv.org/abs/1710.03740.  
[43] High-Resolution Image Synthesis with Latent Diffusion Models. https://arxiv.org/abs/2112.10752.  
[44] Training Deep Nets with Sublinear Memory Cost. https://arxiv.org/abs/1604.06174.    
[45] Automatic Mixed Precision. https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html.  
[46] Model parallelism. https://huggingface.co/docs/transformers/v4.17.0/en/parallelism.  
[47] Paradigms of Parallelism. https://colossalai.org/docs/concepts/paradigms_of_parallelism/.  
[48] Tensor Parallelism tutorial. https://pytorch.org/tutorials/intermediate/TP_tutorial.html.  
[49] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. https://arxiv.org/abs/1910.02054.  
[50] Introducing PyTorch Fully Sharded Data Parallel (FSDP) API. https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/.  
[51] Beam search. https://en.wikipedia.org/wiki/Beam_search.  
[52] Top-k sampling. https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p.  
[53] Model monitoring for ML in production. https://www.evidentlyai.com/ml-in-production/model-monitoring.

---
## Chapter 2: Gmail Smart Compose
## Chapter 3: Google Translate
## Chapter 4: ChatGPT: Personal Assistant Chatbot
## Chapter 5: Image Captioning
## Chapter 6: Retrieval-Augmented Generation
## Chapter 7: Realistic Face Generation
## Chapter 8: High-Resolution Image Synthesis
## Chapter 9: Text-to-Image Generation
## Chapter 10: Personal Headshot Generation
## Chapter 11: Text-to-Video Generation
